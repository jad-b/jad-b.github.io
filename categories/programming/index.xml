<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on while true: continue</title>
    <link>http://jad-b.github.io/categories/programming/</link>
    <description>Recent content in Programming on while true: continue</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 May 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://jad-b.github.io/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Testing Timeouts In Go With Channel Selects</title>
      <link>http://jad-b.github.io/post/Testing%20Timeouts%20in%20Go/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://jad-b.github.io/post/Testing%20Timeouts%20in%20Go/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    &amp;quot;net&amp;quot;
    &amp;quot;testing&amp;quot;
    &amp;quot;time&amp;quot;
)

func TestAddrResolve(t *testing.T) {
    network, addr := &amp;quot;ip4&amp;quot;, &amp;quot;127.0.0.125:44151&amp;quot;
    addrChan := make(chan error)

    // Attempt to resolve IP addr
    go func(ch chan error) {
        _, err := net.DialTimeout(network, addr, 1 * time.Second)
        addrChan &amp;lt;- err
    }(addrChan)

    // Now, see who returns a msg first
    select {
    case e := &amp;lt;-addrChan:
        if e == nil {
            t.Fatalf(&amp;quot;%s://%s should fail to resolve&amp;quot;, network, addr)
        } else if testing.Verbose() { // Success!
            t.Logf(&amp;quot;Call to %s://%s timed out.\nError\n\t%s&amp;quot;, network, addr, e)
        }
    case &amp;lt;-time.After(1 * time.Second):
        t.Fatal(&amp;quot;Address resolution failed to timeout in one second.&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Testing Distributed Systems</title>
      <link>http://jad-b.github.io/post/Testing%20Distributed%20Systems/</link>
      <pubDate>Tue, 19 Apr 2016 14:52:38 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Testing%20Distributed%20Systems/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;TL;DR: Takeaways&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Always, always, always handle errors appropriately. No &lt;code&gt;pass&lt;/code&gt;, no &lt;code&gt;/* TODO
*/&lt;/code&gt;. &lt;em&gt;Something&lt;/em&gt; in the chain needs to verify it&amp;rsquo;s handled.&lt;/li&gt;
&lt;li&gt;Using 3 nodes lets you reproduce 98% of error cases in distributed systems.&lt;/li&gt;
&lt;li&gt;77% of catastrophic failures can be reproduced through unit tests&lt;/li&gt;
&lt;li&gt;Log aggressively, and on both sides of events (message passing).&lt;/li&gt;
&lt;li&gt;The big 5 error-ing events:

&lt;ol&gt;
&lt;li&gt;Startup&lt;/li&gt;
&lt;li&gt;Writes from client&lt;/li&gt;
&lt;li&gt;Node down/unreachable&lt;/li&gt;
&lt;li&gt;Configuration change&lt;/li&gt;
&lt;li&gt;Node join&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a work project involving multiple moving pieces begins the move from
proof-of-concept to preparing for production traffic, the various
pieces are beginning to knit into a whole. In particular, a client-driven event
requires that a list of registered services receive an update. Simple enough,
but failure in this system would result in bad API traffic routing, or worse,
all APIs becoming externally unavailable. Undesirable!&lt;/p&gt;

&lt;p&gt;My previous experiences involved nothing more distributed than your basic
web-server=&amp;gt;DB setup, so I took this as an opportunity to learn from other&amp;rsquo;s
mistakes. Searching around turns up the following advice:&lt;/p&gt;

&lt;h3 id=&#34;simple-testing-can-prevent-most-critical-failures:fab8ffd47911301da93228e4cb7d5640&#34;&gt;Simple Testing Can Prevent Most Critical Failures&lt;/h3&gt;

&lt;p&gt;A whitepaper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; out of the University of Toronto with some incredible
statistics on avoiding the worst-of-the-bad: catastrophic failures&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. They
attribute 92% of CFs to bad error handling, with a further breakdown of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;35% due to

&lt;ul&gt;
&lt;li&gt;Catching but not doing anything about the error&lt;/li&gt;
&lt;li&gt;Aborting on an overly-general error (java&amp;rsquo;s &lt;code&gt;Throwable&lt;/code&gt;, Python&amp;rsquo;s
&lt;code&gt;except:&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;A TODO/FIXME in place, but no handling&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And 23% on aborting on a non-fatal error (failed to delete a temporary file)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;77%&lt;/strong&gt; of these failures they could reproduce using only unit tests. Admittedly,
this is their example unit test:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void testLogRollAfterSplitStart {
    // Create HBase cluster with 1 master and 2 Region Servers
    startMiniCluster();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;which may stretch your imagining of unit tests. I believe what they were
getting at is that the problems are testable within the scope of a single
function&amp;rsquo;s setup/run/cleanup scope. Also, when your definition of unit test is
&amp;ldquo;code that I wrote&amp;rdquo;, and the code that you wrote was HBase, that&amp;rsquo;s quite the
scope.&lt;/p&gt;

&lt;p&gt;Oh, and how about this: &lt;strong&gt;98% of problems could be recreated using no more than
3 nodes&lt;/strong&gt;. Your 120 node Cassandra cluster&amp;rsquo;s dying? Odds are, you only need
three players to recreate it locally.&lt;/p&gt;

&lt;p&gt;An interesting point of difference the author&amp;rsquo;s noted between distributed and
non-distributed systems was that distributed systems tend to have much better
logging. As such, 84% of the studied failures had their triggering events
logged. They logged so much that the author&amp;rsquo;s recommended more advanced log
analysis techniques than a simple &lt;code&gt;grep ERROR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And to wrap this up: Starting up was the most dangerous time for a process, as
summarized under &amp;ldquo;Lessons Learned&amp;rdquo;. More important is to take that list and mix
it up - 90% of the failures could be categorized as a permutation of only three
key events. Just two events interacting accounted for 50% of CFs.&lt;/p&gt;

&lt;h4 id=&#34;further-reading:fab8ffd47911301da93228e4cb7d5640&#34;&gt;Further reading&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;ConfErr - tests configuration errors within a realistic range&lt;/li&gt;
&lt;li&gt;MODIST - Model checking for distributed system&lt;/li&gt;
&lt;li&gt;FATE and DESTINI - Framework for cloud recovery testing&lt;/li&gt;
&lt;li&gt;This looks interesting: KLEE - a code-coverage generator for C programs.
  Can&amp;rsquo;t find any examples for Python though, which would be my use-case.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf&#34;&gt;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;Failure of the system for a majority to all users.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>