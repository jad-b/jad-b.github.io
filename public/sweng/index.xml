<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Swengs on while true: continue</title>
    <link>http://jad-b.github.io/sweng/</link>
    <description>Recent content in Swengs on while true: continue</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jan 2017 09:27:41 -0500</lastBuildDate>
    <atom:link href="http://jad-b.github.io/sweng/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>jit</title>
      <link>http://jad-b.github.io/sweng/jit/</link>
      <pubDate>Wed, 04 Jan 2017 09:27:41 -0500</pubDate>
      
      <guid>http://jad-b.github.io/sweng/jit/</guid>
      <description>&lt;p&gt;Just-in-time (JIT) compilation is a popular compiler performance optimization.
From what I can tell, it&amp;rsquo;s built around profiling runtime behavior, then
converting &amp;ldquo;hotspots&amp;rdquo;, or frequently-used code areas, into optimized machine
code.&lt;/p&gt;

&lt;p&gt;In case you, and &amp;ldquo;you&amp;rdquo; here also means future me, have forgotten, languages are
not always directly converted into machine code, encoded in binary.  as the
exact format is platform-dependent. Instead, code often gets compiled down to an
intermediate representation called &amp;lsquo;bytecode&amp;rsquo;, which is then &lt;em&gt;interpreted&lt;/em&gt; by a
virtual machine (a la the JVM, Python&amp;rsquo;s VM, the &lt;a href=&#34;http://llvm.org/&#34;&gt;LLVM&lt;/a&gt;, etc.) into the actual
machine code. The bytecode contains instructions and values, all the necessary
information to be picked up for further conversion.&lt;/p&gt;

&lt;p&gt;For when I return to research on compilers and JIT specifically, here&amp;rsquo;s some
resources:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compilers&lt;/strong&gt;
- &lt;a href=&#34;http://stackoverflow.com/questions/466790/assembly-code-vs-machine-code-vs-object-code&#34;&gt;Assembly vs. Machine Code&lt;/a&gt;
- &lt;a href=&#34;https://www.toptal.com/python/why-are-there-so-many-pythons&#34;&gt;Overview of Python&amp;rsquo;s VM&lt;/a&gt;. The first %15 is also an excellent refresher on the
  lex,parse,compile-&amp;gt;bytecode process in general.
- &lt;a href=&#34;http://stackoverflow.com/a/3614928/2246784&#34;&gt;SO answer on how a lexer and parser are &lt;em&gt;similar&lt;/em&gt;&lt;/a&gt;
- &lt;a href=&#34;http://aosabook.org/en/500L/a-python-interpreter-written-in-python.html&#34;&gt;Writing a Python VM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JIT&lt;/strong&gt;
- &lt;a href=&#34;https://morepypy.blogspot.com/2009/03/applying-tracing-jit-to-interpreter.html&#34;&gt;How &amp;amp; Why JIT works&lt;/a&gt;
- &lt;a href=&#34;https://andreasgal.com/2008/08/22/tracing-the-web/&#34;&gt;More on JIT tracing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lessons in Python</title>
      <link>http://jad-b.github.io/sweng/Lessons%20in%20Python/</link>
      <pubDate>Tue, 20 Dec 2016 17:21:48 -0500</pubDate>
      
      <guid>http://jad-b.github.io/sweng/Lessons%20in%20Python/</guid>
      <description>&lt;p&gt;Python&amp;rsquo;s versioning scheme is a bit at odds with what you might expect. Upload a
package with version &lt;code&gt;0.1.3-dev4&lt;/code&gt;, and when you go to &lt;code&gt;pip install
mypkg==0.1.3-dev4&lt;/code&gt;, it normalizes it to &lt;code&gt;0.1.3.dev4&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vice Driven Deployment</title>
      <link>http://jad-b.github.io/sweng/Concept%20for%20a%20Vice-Driven%20Deployment%20System/</link>
      <pubDate>Fri, 29 Jul 2016 10:43:26 -0400</pubDate>
      
      <guid>http://jad-b.github.io/sweng/Concept%20for%20a%20Vice-Driven%20Deployment%20System/</guid>
      <description>

&lt;p&gt;Here me out: we could totally write a deployment service that leveraged each of the seven cardinal sins.
Here&amp;rsquo;s my take on it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pride - Deploys code&lt;/li&gt;
&lt;li&gt;Envy - Manages config&lt;/li&gt;
&lt;li&gt;Wrath - Runs functional/smoke tests&lt;/li&gt;
&lt;li&gt;Lust - External service integration. &amp;lsquo;Cause it&amp;rsquo;s, like, promiscuous.&lt;/li&gt;
&lt;li&gt;Sloth - Deployment automation. Depends on pride.&lt;/li&gt;
&lt;li&gt;Greed - Stream processing &amp;amp; data storage.&lt;/li&gt;
&lt;li&gt;Gluttony - Data visualization; &amp;ldquo;feast your eyes&amp;rdquo;. Feeds off of greed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throw this behind a CLI &amp;amp; chat bot, and ta-da, you&amp;rsquo;re a &lt;code&gt;vice pride deploy
&amp;lt;project&amp;gt;&lt;/code&gt; away from releasing code.&lt;/p&gt;

&lt;p&gt;In a little more detail:&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s with deploy|pride. This tool needs to take code artifacts from &lt;em&gt;X&lt;/em&gt; and deploy them onto
system &lt;em&gt;Y&lt;/em&gt;. Deploy is a simplified interface for triggering your configuration management, like
Ansible or Chef, or your container management, like Docker Swarm or Kubernetes. It restricts the API
to just deployment capabilities, so starting &amp;amp; stopping services. It concerns itself with applying
configuration to one of the aforementioned tools in a reliable manner.&lt;/p&gt;

&lt;p&gt;But a deploy isn&amp;rsquo;t enough; you need to verify it worked. That&amp;rsquo;s where &amp;ldquo;verify|wrath&amp;rdquo; is used. Wrath
runs a set of acceptance tests against your system, and its output can be used as feedback to the
deploy. It can also be scheduled to run &lt;em&gt;all&lt;/em&gt; the time, effectively mimicing customer traffic. Bonus
features: replayability of transaction sequences for debugging and probabilistic traffic generation.&lt;/p&gt;

&lt;p&gt;Deploying and testing generates useful data, data that can be used to audit your infrastructure,
monitor changes, and trigger events. That&amp;rsquo;s why it all has to get fed into &amp;ldquo;dataproc|greed&amp;rdquo;, the
stream processing &amp;amp; storage backend. Essentially, dataproc takes events as input, and either 1)
transforms them for storage into whatever storage backends there are, or 2) triggers an integration.
In fact, 1) could be viewed as a special case of 2. The use of events as the system primitive
provides an umbrella abstraction for both of these, which make it so powerful, but more on that
later.
    With great systems like Kafka for this, it wouldn&amp;rsquo;t make a lot of sense to try to beat it at
its own game. What dataproc can offer is a &lt;code&gt;vice&lt;/code&gt;-tuned processing layer either before or after it
hits a system like Kafka. Or maybe you don&amp;rsquo;t want to run kafka just to use &lt;code&gt;vice&lt;/code&gt;; a simple built-in
model could suffice.&lt;/p&gt;

&lt;p&gt;As far as &amp;ldquo;trigger an integration&amp;rdquo;, that falls under &amp;ldquo;lust|notify&amp;rdquo;, and the possibilities are
endless. Certainly you&amp;rsquo;ll want notifications for chat services, maybe emails, possibly updating
issues in your project management software of choice. But you could trigger an AWS Lambda function,
or kick off a data analysis job, or a fresh slew of &amp;ldquo;wrath&amp;rdquo; tests, or a custom binary/script - we
can only guess as to the needs of the future. &amp;ldquo;lust&amp;rdquo; would provide at most a framework, and maybe as
little as an interface for hooking into &lt;code&gt;vice&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With great data, comes the need for pretty pictures. It&amp;rsquo;s a poor system that gives you little
insight into how things are running. This piece gets delivered by &lt;code&gt;gluttony|dataviz&lt;/code&gt;. Enough said?
It&amp;rsquo;s a dashboard. Enough said.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;config|envy&amp;rdquo; sits in interesting contrast to &amp;ldquo;dataproc|greed&amp;rdquo;; if &amp;ldquo;dataproc&amp;rdquo; is for data
&lt;em&gt;generated&lt;/em&gt; by the system, &amp;ldquo;config&amp;rdquo; is for data &lt;em&gt;given&lt;/em&gt; to the system by the users. After all, you
have to store configuration and secrets somewhere. &amp;ldquo;config&amp;rdquo; would be responsible for isolating and
retrieving data in the correct format.&lt;/p&gt;

&lt;p&gt;Between retrieving configuration, code artifacts, executing configuration software, and running
verification tests, every project is likely to become its own snowflake. &amp;ldquo;auto|sloth&amp;rdquo; speaks to the
need to script and automate your deployments. Behind the scenes, all of those actions can be viewed
as a graph, where each vertice represents an action, and the edges represent the flow of data.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s talk for a minute about events. At a minimum, information sent to &amp;ldquo;dataproc&amp;rdquo; needs to be
encoded as messages/event notifications. However, I&amp;rsquo;m not convinved the entire system must be
event-driven. For one, I don&amp;rsquo;t believe each of the above seven services would be a running service.
In the case of I am open to the possibility of using them throughout, and perhaps experience will
show this to be best.&lt;/p&gt;

&lt;h3 id=&#34;but-why-this-is-silly:09d83bf38a40a8e0da5bb73a84b36e60&#34;&gt;But why?/This is silly.&lt;/h3&gt;

&lt;p&gt;Deploying and operationalizing code is often harder than writing the code in the first place. And it
when it comes to monitoring the state of your infrastructure, it feels like you&amp;rsquo;re trying to
interrogate a child you&amp;rsquo;ve entrusted all of your production configuration too: &amp;ldquo;What are the
versions running for this service?&amp;rdquo; met by &amp;ldquo;I want candy! Also, that&amp;rsquo;ll be a for-loop followed by
grep piped to sed piped to cut&amp;hellip;&amp;rdquo; What the hell? The information&amp;rsquo;s there! But you either never made
it machine-consumable, or you never taught the machine how to consume it.&lt;/p&gt;

&lt;h3 id=&#34;postface:09d83bf38a40a8e0da5bb73a84b36e60&#34;&gt;Postface.&lt;/h3&gt;

&lt;p&gt;Now, I&amp;rsquo;m going to be partial to this because I&amp;rsquo;m in love with the cleverness of
my own ideas.  You&amp;rsquo;d have to make it good, and you&amp;rsquo;d have to make it
future-proof. This is how I&amp;rsquo;d do it:&lt;/p&gt;

&lt;p&gt;Write the interface first. If you&amp;rsquo;ve mastered the deploy of &lt;em&gt;any&lt;/em&gt; piece of
reasonably complex software (a.k.a. greater than just &lt;code&gt;scp&lt;/code&gt;-ing files to a
box), you know what you want from a deployment assistant. Code up that
interface, and stub out the functionality as you go. I&amp;rsquo;d want something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Retrieve configuration for the deploy; could subsume within the next step
vice envy &amp;lt;options...&amp;gt; &amp;lt;project&amp;gt;
# Deploy our project!
vice pride deploy &amp;lt;project&amp;gt;
# Run our post-release smoke tests
vice wrath &amp;lt;project&amp;gt; &amp;lt;options...&amp;gt;
# Visualize how our deploy went
vice gluttony report &amp;lt;project&amp;gt;
# See how _all_ our projects are looking
vice gluttony report &amp;lt;saved dashboard name&amp;gt;
# Drop a message in Slack about the release
vice lust project -m &amp;quot;Everything&#39;s looking good, guys&amp;quot;
# Backup our deploy data into S3
vice greed backup &amp;lt;options...&amp;gt; &amp;lt;S3 URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which means I&amp;rsquo;d need a way of making &lt;code&gt;vice&lt;/code&gt; (I guess the collective name of
this tool is &lt;code&gt;vice&lt;/code&gt;? Is now.) aware of &lt;code&gt;&amp;lt;project&amp;gt;&lt;/code&gt;. Communication is either
push- or pull-based, so it&amp;rsquo;s either that &lt;code&gt;vice&lt;/code&gt; know&amp;rsquo;s how to find projects, or
projects register themselves with &lt;code&gt;vice&lt;/code&gt;. But, you know what? &lt;em&gt;That&lt;/em&gt; matters
far less than what I want my interaction with &lt;code&gt;vice&lt;/code&gt; to be like. Both models
would work, just like Chef and Ansible work. If one&amp;rsquo;s better than the other,
and we start with the lesser, it&amp;rsquo;ll be our good engineering practices of
writing loosely-coupled, modular code that saves us headaches during the refactor.&lt;/p&gt;

&lt;p&gt;Which brings me to modularity. If you want code to survive over time, it has to
be modular, which in this context means we need to be able to write new
backends for emerging technologies that satisfy the same functional interfaces.
Was that a mouthful? Try this: We want &lt;code&gt;vice&lt;/code&gt; to be compatible with new tech.
Not in an auto-magical &amp;ldquo;&lt;code&gt;vice&lt;/code&gt; will work with &lt;em&gt;any database ever&lt;/em&gt;&amp;rdquo; way, but
rather we know what data storage requirements we&amp;rsquo;ll have for &lt;code&gt;greed&lt;/code&gt;, which
means we can write adapters to hide whether we&amp;rsquo;re storing it in Postgresql,
DynamoDB, a file system, or HDFS.&lt;/p&gt;

&lt;p&gt;Maybe we won&amp;rsquo;t be able to completely satisfy our DataStorer interface with each
backend - that&amp;rsquo;s okay! Do a &lt;code&gt;raise NotImplemented&lt;/code&gt;, and allow your calling code
to deal with partial interfaces in non-critical functionality. Obviously you
need to read &amp;amp; write data, but maybe you can&amp;rsquo;t provide the same level of
searching you&amp;rsquo;d like across all backends. Okay. Life is a moving target. Choose
your trade-offs, and above all stay flexible through modular code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing Timeouts In Go With Channel Selects</title>
      <link>http://jad-b.github.io/sweng/Testing%20Timeouts%20in%20Go/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://jad-b.github.io/sweng/Testing%20Timeouts%20in%20Go/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    &amp;quot;net&amp;quot;
    &amp;quot;testing&amp;quot;
    &amp;quot;time&amp;quot;
)

func TestAddrResolve(t *testing.T) {
    network, addr := &amp;quot;ip4&amp;quot;, &amp;quot;127.0.0.125:44151&amp;quot;
    addrChan := make(chan error)

    // Attempt to resolve IP addr
    go func(ch chan error) {
        _, err := net.DialTimeout(network, addr, 1 * time.Second)
        addrChan &amp;lt;- err
    }(addrChan)

    // Now, see who returns a msg first
    select {
    case e := &amp;lt;-addrChan:
        if e == nil {
            t.Fatalf(&amp;quot;%s://%s should fail to resolve&amp;quot;, network, addr)
        } else if testing.Verbose() { // Success!
            t.Logf(&amp;quot;Call to %s://%s timed out.\nError\n\t%s&amp;quot;, network, addr, e)
        }
    case &amp;lt;-time.After(1 * time.Second):
        t.Fatal(&amp;quot;Address resolution failed to timeout in one second.&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Testing Distributed Systems</title>
      <link>http://jad-b.github.io/sweng/Testing%20Distributed%20Systems/</link>
      <pubDate>Tue, 19 Apr 2016 14:52:38 -0400</pubDate>
      
      <guid>http://jad-b.github.io/sweng/Testing%20Distributed%20Systems/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;TL;DR: Takeaways&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Always, always, always handle errors appropriately. No &lt;code&gt;pass&lt;/code&gt;, no &lt;code&gt;/* TODO
*/&lt;/code&gt;. &lt;em&gt;Something&lt;/em&gt; in the chain needs to verify it&amp;rsquo;s handled.&lt;/li&gt;
&lt;li&gt;Using 3 nodes lets you reproduce 98% of error cases in distributed systems.&lt;/li&gt;
&lt;li&gt;77% of catastrophic failures can be reproduced through unit tests&lt;/li&gt;
&lt;li&gt;Log aggressively, and on both sides of events (message passing).&lt;/li&gt;
&lt;li&gt;The big 5 error-ing events:

&lt;ol&gt;
&lt;li&gt;Startup&lt;/li&gt;
&lt;li&gt;Writes from client&lt;/li&gt;
&lt;li&gt;Node down/unreachable&lt;/li&gt;
&lt;li&gt;Configuration change&lt;/li&gt;
&lt;li&gt;Node join&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a work project involving multiple moving pieces begins the move from
proof-of-concept to preparing for production traffic, the various
pieces are beginning to knit into a whole. In particular, a client-driven event
requires that a list of registered services receive an update. Simple enough,
but failure in this system would result in bad API traffic routing, or worse,
all APIs becoming externally unavailable. Undesirable!&lt;/p&gt;

&lt;p&gt;My previous experiences involved nothing more distributed than your basic
web-server=&amp;gt;DB setup, so I took this as an opportunity to learn from other&amp;rsquo;s
mistakes. Searching around turns up the following advice:&lt;/p&gt;

&lt;h3 id=&#34;simple-testing-can-prevent-most-critical-failures:fab8ffd47911301da93228e4cb7d5640&#34;&gt;Simple Testing Can Prevent Most Critical Failures&lt;/h3&gt;

&lt;p&gt;A whitepaper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; out of the University of Toronto with some incredible
statistics on avoiding the worst-of-the-bad: catastrophic failures&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. They
attribute 92% of CFs to bad error handling, with a further breakdown of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;35% due to

&lt;ul&gt;
&lt;li&gt;Catching but not doing anything about the error&lt;/li&gt;
&lt;li&gt;Aborting on an overly-general error (java&amp;rsquo;s &lt;code&gt;Throwable&lt;/code&gt;, Python&amp;rsquo;s
&lt;code&gt;except:&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;A TODO/FIXME in place, but no handling&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And 23% on aborting on a non-fatal error (failed to delete a temporary file)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;77%&lt;/strong&gt; of these failures they could reproduce using only unit tests. Admittedly,
this is their example unit test:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void testLogRollAfterSplitStart {
    // Create HBase cluster with 1 master and 2 Region Servers
    startMiniCluster();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;which may stretch your imagining of unit tests. I believe what they were
getting at is that the problems are testable within the scope of a single
function&amp;rsquo;s setup/run/cleanup scope. Also, when your definition of unit test is
&amp;ldquo;code that I wrote&amp;rdquo;, and the code that you wrote was HBase, that&amp;rsquo;s quite the
scope.&lt;/p&gt;

&lt;p&gt;Oh, and how about this: &lt;strong&gt;98% of problems could be recreated using no more than
3 nodes&lt;/strong&gt;. Your 120 node Cassandra cluster&amp;rsquo;s dying? Odds are, you only need
three players to recreate it locally.&lt;/p&gt;

&lt;p&gt;An interesting point of difference the author&amp;rsquo;s noted between distributed and
non-distributed systems was that distributed systems tend to have much better
logging. As such, 84% of the studied failures had their triggering events
logged. They logged so much that the author&amp;rsquo;s recommended more advanced log
analysis techniques than a simple &lt;code&gt;grep ERROR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And to wrap this up: Starting up was the most dangerous time for a process, as
summarized under &amp;ldquo;Lessons Learned&amp;rdquo;. More important is to take that list and mix
it up - 90% of the failures could be categorized as a permutation of only three
key events. Just two events interacting accounted for 50% of CFs.&lt;/p&gt;

&lt;h4 id=&#34;further-reading:fab8ffd47911301da93228e4cb7d5640&#34;&gt;Further reading&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;ConfErr - tests configuration errors within a realistic range&lt;/li&gt;
&lt;li&gt;MODIST - Model checking for distributed system&lt;/li&gt;
&lt;li&gt;FATE and DESTINI - Framework for cloud recovery testing&lt;/li&gt;
&lt;li&gt;This looks interesting: KLEE - a code-coverage generator for C programs.
  Can&amp;rsquo;t find any examples for Python though, which would be my use-case.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf&#34;&gt;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fab8ffd47911301da93228e4cb7d5640:whitepaper&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;Failure of the system for a majority to all users.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fab8ffd47911301da93228e4cb7d5640:catfail&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://jad-b.github.io/sweng/DDoS%20Simulator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://jad-b.github.io/sweng/DDoS%20Simulator/</guid>
      <description>

&lt;h1 id=&#34;thesis:9731617ea594a91f6f813d8b19dc4739&#34;&gt;Thesis&lt;/h1&gt;

&lt;p&gt;Execute and record internet traffic scenarios.&lt;/p&gt;

&lt;h1 id=&#34;summary:9731617ea594a91f6f813d8b19dc4739&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;An ideal test of traffic control combines both black-box testing and white-box
monitoring. This requires being both in control of the traffic sources and
sinks. Traffic sources can be orchestrated to execute Scenarios, instructions
answering the questions &amp;ldquo;what kind, how much, how often, and where&amp;rdquo; where
sending traffic, recording whether the traffic succeeded, failed, or is pending.
Sinks listen for and record Scenario traffic. Analysis of the Scenario&amp;rsquo;s outcome
is available for later analysis.&lt;/p&gt;

&lt;h1 id=&#34;explanation:9731617ea594a91f6f813d8b19dc4739&#34;&gt;Explanation&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;The Problem&lt;/li&gt;
&lt;li&gt;The Solution&lt;/li&gt;
&lt;li&gt;Components

&lt;ul&gt;
&lt;li&gt;Scenario&lt;/li&gt;
&lt;li&gt;Source&lt;/li&gt;
&lt;li&gt;Sink&lt;/li&gt;
&lt;li&gt;Orchestrator&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Leveraging existing monitoring&lt;/li&gt;
&lt;li&gt;Post-Scenario Analysis&lt;/li&gt;
&lt;li&gt;Outstanding Questions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DDoS is a crippling yet easily-obtained capability that threatens any internet
presence. Recent examples include the Playstation Network, the journalist
Brian Krebs, and Dyn, a DNS provider. Importantly, the recent attack on Dyn showed how
single points of failure, such as DNS, can result in multi-company impact for a
single attack. With companies for whom their online accessibility if critical to
their services, correctly handling identified malicious traffic is key to their
continued existence. This assumption, when coupled with the mantra &amp;ldquo;that which
is untested is broken&amp;rdquo;, implies that without realistic yet safe means of
simulating malicious traffic, all preparations aren&amp;rsquo;t proven effective until the
next attack. Can we do better?&lt;/p&gt;

&lt;p&gt;Simluating DDoS attacks requires a few&lt;/p&gt;

&lt;p&gt;Clients record route traffic into Client x Route tables. The cell of each table
holds the State of the traffic: Pending, Success, or Failed. Metadata concerning
the actual sending of the traffic is indexed by a FK of (Client, Route).&lt;/p&gt;

&lt;p&gt;Similar to Clients, Hosts store the State of route traffic in a Host x Route
table. Given an arbitrary number of intermediate steps between a client and its
destination, which a &lt;code&gt;traceroute&lt;/code&gt; to a any given public service can display,
hosts may not be able to easily identify whether or not they &lt;em&gt;should&lt;/em&gt; receive a
given Route&amp;rsquo;s traffic. Some introspection may be possible for things like HTTP,
where the Host: header can be examined, but other protocols may not offer such
clues. Thus, (Host,Route) State must &lt;em&gt;assume&lt;/em&gt; that they&amp;rsquo;re not meant to receive
traffic and begin in the Success state, changing to Failure upon receipt of
traffic.&lt;/p&gt;

&lt;h1 id=&#34;excrutiating-detail:9731617ea594a91f6f813d8b19dc4739&#34;&gt;Excrutiating Detail&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://jad-b.github.io/sweng/Monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://jad-b.github.io/sweng/Monitoring/</guid>
      <description>

&lt;h2 id=&#34;resources:7eae574de37890585781363c1ecc3c61&#34;&gt;Resources&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>