+++
categories = ["software"]
date = "2017-05-03T08:29:09-04:00"
tags = ["conference", "data science"]
title = "ODSC2017"

+++

Notes from the [Open Data Science Conference](https://www.odsc.com/boston) in Boston.

<!--more-->

# May 3rd - Trainings
## Intro to & Advanced Deep Learning with DLI
* Speaker: Charles Killam (NVIDIA)
* Labs: Using Neural Networks for image segmentation, recognition, and
  time-series health record text extraction.

When classifying: one input neuron per dimension; one output neuron per feature.
Network size gets referred to as (n-1), as you don't count the input layer.
So a 3-layer network has an input, two hidden, and one output layer(s).

Convolution=Kernel=Filter. Usually, each convolution decreases the output
dimensions. However, by using "padding"(?) you can maintain the original
dimesnions, and deconvolutions can inflate dimensions. There is a notion of
"channels", such as RGB image data having three channels, one per color value.
Convolutions can also create channels. When you say "convolution 1 creates 100
convolution outputs", you are requesting 100 output kernels to be initialized
and applied, creating a 1:100 series of outputs. Combinatorial _explosion_.

There are a few key hyper-parameters: Training set size, batch size, and epoch.
Batch size is the number of data points your framework performs before before
aggregating and backpropagating the loss through the network. Epochs refer to
the number of times the framework trains on your training set. At the end of
each epoch, the network is tested using a validation set. You _should_ see the
overall loss decrease during validation. If it goes up, your model has
__overfit__. Presumably, you could just use the last good values before you
detected overfitting. But it can be trickier than that, as some variation is
natural during training.

What falls on the Data Scientist:
* Hyperparameter selection
* Transfer Learning / Fine-Tuning (Re-using existing trained networks)
* Hidden layer architecture; how many, what size.

Tips:
* Think [10,100] layers when getting started.
* Annealed learning rate is a Good Thing.
* Mean normalization is still required before processing, just like with other
  ML methods.

Keras is a high-level abstraction over neural network frameworks. It's
compatible with both Theano and TensorFlow, so is probably a good level to rest
at. Plus, it has the potential to add the Next Hot Framework as a backend,
saving you rewriting.

Recurrent Neural Networks are appropriate for a broad variety of tasks, such as
video frame classification, audio somethings, NLP, and time series analysis. In
fact, its modeling things _as_ sequences that makes them appropriate for RNNs.
RNNs remember state from previous inputs. Normally, you run into a "vanishing
gradient", as the influence of values diminishes as they age. Using Long
Short-Term Memory units (LSTMs), we can remember theoretically infinitely long
chains.

Resources: cs231n Stanford Classes. Videos 1-4 on Why NNs?, 5-10 on parameter
selection, 11-12 on Recurrent Neural Networks.

### Questions
> How do you visualize the layer features?
TensorFlow can do some of this for you. NVIDIA's DIGITS also supported layer
visualization on top of CAFFE, not sure if it does for others.

# May 4th, Day 1 of Sessions

## Deep Neural Networks using TensorBoard
Let's start from the beginning. Fed data, you model outputs a value per data
point (or a set of values, when classifying). You compare your prediction
against the true value using an _error function_, which outputs a value telling
you how bad a job you did. Often, these don't come out neatly between 0-1, as
probability dictates, so you use a sigmoid/tanh function to compress everything
between [0,1]. The total error of your model needs to account for the error
amongst all your predictions. Maximum Likelihood = Highest product of all
probabilities. But products become tiny and volatile as we get lots of terms.
Instead, we can sum the log of each probability. If we sum the negative
logarithms, then higher probabilities sum to _smaller_ numbers: `log(0.6) -
log(.2) - log(.1) - log(.) = 4.8`. This makes our goal easy: reach zero. With
our error in hand, we calculate derivatives to take us towards lower error, and
apply it using gradient descent (gradient rate sold separately). So we feed in
some data, calculate the error of our predictions, squash them down between
[0,1], and use our error to update our model.

But you knew that. Let's talk LSTMs. LSTMs are full networks in themselves, with
four interacting layers. These layers together are referred to as a cell, and
cells have state. The LSTM has mechanisms for controlling how much new
information affects state called "gates". A gate is a sigmoid function followed
by pointwise multiplication. Values `<=0` aren't allowed through, and values `>=
1` are completely allowed.

The first gate helps the cell to forget. It takes the previous output and
current input, multiplies them by the weights of the gate, adjusts by bias, and
squashes the whole thing through the sigmoid. Thus, we only output values
between [0,1].

<div>$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$</div>

The next gate determines what information we'll store. We need to identify what
values we'll update (`$i_t$`), as well the candidate values for what  we'll
update them with (`$C_t$`).  We sigmoid the values we identify to update, while
we `tanh` the candidate values.

<div>$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \
C_t = tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
$$</div>

With that, we actually need to forget. We forget our prevous cell state through
by multiplying our previous cell state with the forgetting gate layer above, and
sum in our updated cell state we just calculated.

## Analyzing Evictions During the Housing Crisis
Dataiku gives you a graph-based data pipeline design tool that gives you a GUI
interface into most common operations, from ETL to ML. Our instructor used it to
show that poor black people in Missouri get evicted a lot.  I can't but feel the
point was missed.

## Anomaly Detection in Wireless Networks using Mobile Phone Data
Proactive > Reactive. You must look at distributions over data points to
ascertain actual behavior. Once compared to our prior baseline knowledge, we
can identify actively degrading conditions, and use visualizations to determine
the root cause.

Identified "procedure duration", i.e. how long a specific request took, was
the single best indicator for detecting degradation. Having only a single
variable kept analysis performance quick. A request can be "provide me a
service" (service request), or "terminate my service/free the radio channel"
(release request).  Initially tried to analyze e'rything and got nowhere. Key
was to focus on the objective, which led ID'ing the service & release request as
KPIs.

Self-Organizing Maps (SOMs) are a form of unsupervised NNs that produce a low
(like, 2) dimensional representation of the input space. It'd be interesting if
existing NN packages, like TF or Theano, could be tricked into creating SOMs.

## Agenda
* Building Deep NNs using TensorBoard (313) [11,1145]
* Pomegranate: fast and flexible probabilistic modeling in python (311)
  [1145,1230]
* Analyzing Evictions During the Housing Crisis [1330,1500]
* The Cognitive Computing Promise (310)[1500,1545]
* Anomaly Detection in Wireless Networks using Mobile Phone Data (301)
  [1615,1700]

## Wishlist
* Using AI to Answer Science Questions [1145,1230]
* Visualizing the Health of the Internet with Measurement Lab [1145,1230]
* Fuzzy Matching to the Rescue: Aligning Survey Design Across Time (310)
  [1700,1745]
* Utilizing Machine Learning for Malware/Botnet Detection (311) [1630,1800]

# May 5th, Day 2 of Sessions
## Agenda
* Geospatail Voronoi Analysis (310) [9,945]
* Deep Learning for Multivariate Time-series analysis (313) [945,1030]
* Robust  Regression: Solving the challenges posed by Dirty Data (Ballroom A) [11,1145]
* Making Decisions Under Uncertainty Using Bayesian Inference and Stan
  (309) [1145,1230]
* The Biological Path towards Strong AI (310) [1230,1315]
* Interpretability and the Future of ML  (Ballroom A) [1415,1500]
* The Power and Pains of Smartphone Sensor Data (309) [1500,1545]
* From the Trenches: Managing and Building Data Science Teams (309) [1615, 1700]

## Wishlist
* Data Science is Software Development (311) [1330,1500]
* Labeling Medical Documents with ML (313) [1630,1715]
* Behind the scenes of training, managing, and deploying ML models
* Applying DS Tools to Understand Web Traffic (310) [1700,1745]
* The Barbell Effect of ML (309) [1230,1315]
* Telling a Quantitative Story / Corporate Data Science by Angela Bassa

## Deep Learning for Multivariate Time-series analysis (MvTSA)
Desires for TS ML: Classification, Regression, Forecasating, Anomaly Detection.
Remember: Regression is the predicting of values _within the bounds of your
existing data_, not predicting _outside_ the range of what you've seen.

Traditional methods of MvTSA include comparing the euclidean distance between
two series - but it fails to detect the same TS shifted ahead. Dynamic Time
Warping (DTW) can adjust for such things, but it's `$O(n^2)$`, and thus somewhat
expensive. There's Symbolic Representation, where you bin its values with
labels. This turns your TS into a string of bin labels, which lets you use
existing string comparison algos. There's Feature Extraction, using moving avg,
Short Time Fourier Transform (STF), shapelets(?), and dimensionality reduction.

But now we've got RNNs. They're hard to train; their recurrence means that its
current state means depends on all states before it (Real Time BackPropagation
(RTBP). We can "unroll" them into distinct layers, and apply BackPropagation
Through Time (BPTT).

He presented two options here:  Sequence-to-Sequence, and Sequence-to-1. He
mentioned preferring S21 over S2S.

An intuition on ConvNNs: You can think of conv kernels as selecting for which
parts of the image look the most like the kernel itself.

FIR in signal processing == 1D ConvNN.

They're hiring!

## Robust Regression: Solving the challenges posed by Dirty Data
OLS is highly vulnerable to outliers. We want to minimze the influence of
outliers on our loss function. An ideal loss function is Non-negative, minimums
at 0, symmetric, and monotonic. If we convert the problem to weighted least
squares, we get closer; as our a residual grows large, its weight gets smaller.
Thus, we de-influence outliers. This creates a cyclic dependency, _but_
iteratively reweighted least squares gets around this. The most common form of
IRWS is the Huber loss function.  It has a steep increase of error as we grow
larger, so we convert them to straight lines at an arbitrary point. Essentially,
we cap how the size of their weight. The German-McClure and Tukey loss functions
actually remove outliers entirely after a certain point,  zeroing out outlier
influence. This class of outlier-accounting loss functions is called
M-Estimators.

There is another way, called Random sample consensus (RANSAC). Sample M points
from the  dataset. Fit a line, and count the number of inliers of your entire
dataset that fallwithin a residual threshold. Iterate for a time, and select the
best model.

## Making Decisions Under Uncertainty Using Bayesian Inference and Stan
Too much. But I did get a good picture.

![Picture goes here]()

## The Power and Pains of Smartphone Sensor Data
The IMU (Interial Measurement Unit) is built from the gyroscope, accelerometer,
and magnetometer. The gyroscope measures rotation, the magnetometer for
magnetic/electrical fields, and the accelerometer explains itself.

GPS Data Workflow: Raw GPS Data -> Map Provider -> Data Science -> ...
GPS is a battery drain, so casual sampling might be 15-30 seconds

## From the Trenches: Managing and Building Data Science Teams
To build domain knowledge, plan on embedding/rotating the new person with their
pertinent counterpart. Consider these embeddings as a prereq to their
progression.
